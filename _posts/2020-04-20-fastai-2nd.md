---
layout: post
title:  "Fastai2 2nd post"
ref: fastai
categories: jekyll update
lang: en

---

I'm now in 04_mnist_basics of the fastbook tutorial.
After finishing the 02 and the halfway through 04, I want to make few comments about the intitive way of identifying number "3" and "7" described in the tutorial.

To identify "3", the most intitive way is to make a masking board that has a shape of "3". After some kind of preprocessing, like adjusting the image size to 28x28 and place the number in the middle of the image,
we can place the masking board of "3" on the image. If the image indeed contains "3", we exect the image does contain "3" after placing the masking board on the image.
If the image contains other numbers like "7", the masking board would hide most of the "7" and it would become incomprehensible.

So, if the omst of the image is unaffected by the "3" shape of the masking, we can conclude that image contains "3".
In this way, we can place masking boards with "1", "2", "3", and so on and judge which masking board would fit the best for the given image.
This is how a computer can judge the number; we can just prepare with 10 masking boards and let the computer apply them and judge which one had the least impact (unchange) the image.

The problem of this approach is, we need to prepare with these 10 masking boards (basically 28x28 pixel plane with 1/0 intensity describing the shape of 10 digits) a priori.

The initial part of the lesson #4 describes the way to make these masking boards; just average many hand-written "3".
If we have 1,000 28x28 images describing hand-written "3", just add them up and divide by 1,000.

In my understanding so far, this is basically what neural network with one linear layer would do, but in a different machine learning way.

By saying "different way", I mean, neural network visits every single pixel of 28x28 (there are 784 pixels) and change their intensity between 0 and 1.
First, they set the intensity randomly. And during the learning, they change the intensity by pre-determined amout (say 0.1) to the "right" direction.
If it works, they end up with the similar (if not the same) solution to the "average" solution; they start with the random noise 28x28 image but converges to the "average" shape (masking board) of "3".

The key here is how to know the "right" direction during the multiple iterations of learning. This is where the "gradient" comes in, which you see everywhere in deep learning.

I may be wrong but we will see.
